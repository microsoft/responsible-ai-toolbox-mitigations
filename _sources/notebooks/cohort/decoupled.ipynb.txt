{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoupled Classifiers\n",
    "\n",
    "This notebook aims to be a tutorial on how to use the ``DecoupledClass`` estimator, provided in the `raimitigations.cohort` package.\n",
    "This class is based on the work presented in the paper [Decoupled classifiers for group-fair and efficient machine learning](https://www.microsoft.com/en-us/research/publication/decoupled-classifiers-for-group-fair-and-efficient-machine-learning/). The ``DecoupledClass`` estimator will build a different estimator for each cohort, where the cohort separation rules are defined by a set of parameters of the class, which is similar to how the ``CohortManager`` class creates its cohorts. Both ``DecoupledClass`` and ``CohortManager`` inherit from the same abstract class ``CohortHandler``, which implements the core functionalities for handling cohorts. The difference between the ``CohortManager`` and the ``DecoupledClass`` is that the former aims to provide an interface for creating a variety of different pipelines that are executed over each cohort separately (be it a pipeline with an estimator, different pipelines to each cohort, and so on), while the goal of the latter is to function as an estimator, which means that it will always fit a model over each cohort separately, and it can also apply some transformations to each cohort as well, but the transformations must always be the same (it doesn't allow using different pre-processing pipelines to each cohort as it is allowed in the ``CohortManager``).\n",
    "\n",
    "In this notebook, we'll show the different ways we can instantiate and use the ``DecoupledClass``. Let's start off by opening a dataset. Here, we'll use the UCI Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>age</th>\n",
       "      <th>menopause</th>\n",
       "      <th>tumor-size</th>\n",
       "      <th>inv-nodes</th>\n",
       "      <th>node-caps</th>\n",
       "      <th>deg-malig</th>\n",
       "      <th>breast</th>\n",
       "      <th>breast-quad</th>\n",
       "      <th>irradiat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30-39</td>\n",
       "      <td>premeno</td>\n",
       "      <td>30-34</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>20-24</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "      <td>right_up</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>20-24</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>60-69</td>\n",
       "      <td>ge40</td>\n",
       "      <td>15-19</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "      <td>left_up</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "      <td>right_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1</td>\n",
       "      <td>30-39</td>\n",
       "      <td>premeno</td>\n",
       "      <td>30-34</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>left</td>\n",
       "      <td>left_up</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1</td>\n",
       "      <td>30-39</td>\n",
       "      <td>premeno</td>\n",
       "      <td>20-24</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>left</td>\n",
       "      <td>left_up</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>60-69</td>\n",
       "      <td>ge40</td>\n",
       "      <td>20-24</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>right</td>\n",
       "      <td>left_up</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>40-49</td>\n",
       "      <td>ge40</td>\n",
       "      <td>30-34</td>\n",
       "      <td>3-5</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "      <td>50-59</td>\n",
       "      <td>ge40</td>\n",
       "      <td>30-34</td>\n",
       "      <td>3-5</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class    age menopause tumor-size inv-nodes node-caps  deg-malig breast  \\\n",
       "0        0  30-39   premeno      30-34       0-2        no          3   left   \n",
       "1        0  40-49   premeno      20-24       0-2        no          2  right   \n",
       "2        0  40-49   premeno      20-24       0-2        no          2   left   \n",
       "3        0  60-69      ge40      15-19       0-2        no          2  right   \n",
       "4        0  40-49   premeno        0-4       0-2        no          2  right   \n",
       "..     ...    ...       ...        ...       ...       ...        ...    ...   \n",
       "281      1  30-39   premeno      30-34       0-2        no          2   left   \n",
       "282      1  30-39   premeno      20-24       0-2        no          3   left   \n",
       "283      1  60-69      ge40      20-24       0-2        no          1  right   \n",
       "284      1  40-49      ge40      30-34       3-5        no          3   left   \n",
       "285      1  50-59      ge40      30-34       3-5        no          3   left   \n",
       "\n",
       "    breast-quad irradiat  \n",
       "0      left_low       no  \n",
       "1      right_up       no  \n",
       "2      left_low       no  \n",
       "3       left_up       no  \n",
       "4     right_low       no  \n",
       "..          ...      ...  \n",
       "281     left_up       no  \n",
       "282     left_up      yes  \n",
       "283     left_up       no  \n",
       "284    left_low       no  \n",
       "285    left_low       no  \n",
       "\n",
       "[286 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uci_dataset as database\n",
    "\n",
    "from raimitigations.utils import split_data\n",
    "import raimitigations.dataprocessing as dp\n",
    "from raimitigations.cohort import DecoupledClass, fetch_cohort_results\n",
    "\n",
    "df = database.load_breast_cancer()\n",
    "label_col = \"Class\"\n",
    "df[label_col] = df[label_col].replace({\t\"recurrence-events\": 1, \n",
    "\t\t\t\t\t\t\t\t\t\t\"no-recurrence-events\": 0})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df, label=\"Class\", test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Scenario\n",
    "\n",
    "Let's consider the following scenario: suppose that we want to train a different model for each cohort comprised of the different values in the ``irradiat`` column. To do this, we can call the ``DecoupledClass`` using the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COHORTS\n",
      "cohort_0:\n",
      "\tSize: 173\n",
      "\tQuery:\n",
      "\t\t(`irradiat` == \"no\")\n",
      "\tValue Counts:\n",
      "\t\t0: 130 (75.14%)\n",
      "\t\t1: 43 (24.86%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_1:\n",
      "\tSize: 55\n",
      "\tQuery:\n",
      "\t\t(`irradiat` == \"yes\")\n",
      "\tValue Counts:\n",
      "\t\t0: 30 (54.55%)\n",
      "\t\t1: 25 (45.45%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing = [dp.BasicImputer(verbose=False), dp.EncoderOrdinal(verbose=False)]\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "\t\t\t\t\tcohort_col=[\"irradiat\"], \n",
    "\t\t\t\t\ttransform_pipe=preprocessing\n",
    "\t\t\t\t)\n",
    "dec_class.fit(X_train, y_train)\n",
    "\n",
    "dec_class.print_cohorts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``cohort_col`` parameter works similarly to the same parameter in the ``CohortManager`` class: it creates a different cohort for each combination of values found in the columns specified in the ``cohort_col`` list (check this [notebook for more details](./cohort_manager.ipynb)). Therefore, since ``cohort_col`` = [\"irradiat\"], then we'll create one cohort for all instances where the `irradiat` column is \"no\", and another column where its value is \"yes\". We then train two models: one for each cohort. Since no estimators were provided, we'll create a copy of the baseline estimator used by the ``DecoupledClass``: a ``sklearn.tree.DecisionTreeClassifier`` for classification problems, or a ``sklearn.tree.DecisionTreeRegressor`` for regression.\n",
    "\n",
    "Note that we also provided a pre-processing pipeline through the ``transform_pipe`` parameter. What happens here is that each cohort will have their own copy of this pipeline, and before fitting the model, each cohort's dataset (a subset of the original dataset) will go through this pipeline. In this case, before fitting the model, each cohort will impute the missing values, and then encode the categorical features. Differently from the ``CohortManager`` class, the ``DecoupledClass`` doesn't allow to use different pipelines for each cohort: all cohorts will use different copies of the same pipeline.\n",
    "\n",
    "After creating the ``DecoupledClass`` object, we can then print some information about each cohort created. To do this, we use the ``print_cohorts()`` method.\n",
    "\n",
    "## Merging Invalid Cohorts\n",
    "\n",
    "When creating multiple cohorts, we might end up with a few cohorts with a skewed label distribution, or very small cohorts. In these cases, we might want to fix these cohorts before proceeding. One approach is to use data rebalancing and create new instances for only a few cohorts, and we can do this using the [dataprocessing.Synthesizer](../dataprocessing/module_tests/rebalance_sdv.ipynb) class or using the [Rebalance](../dataprocessing/module_tests/rebalance_imbl.ipynb) class together with the ``CohortManager()``. Apart from these solutions, the ``DecoupledClass`` also offers some new solutions. The first solution, which we'll explore in this section, is to greedily merge invalid cohorts until they become valid. \n",
    "\n",
    "When merging cohorts, we first need to define what a valid cohort is. Here, we consider that invalid cohorts are those that fall into at least one of the following conditions:\n",
    "\n",
    "1. **Small Cohorts:** cohorts with a size below a certain threshold\n",
    "2. **Skewed Cohorts:** cohorts with a label column with a skewed distribution.\n",
    "\n",
    "After a cohort is deemed invalid, we need to decide which cohort it will be merged into. We simply choose the smallest cohort different from the invalid cohort and then merge them (this is why we mentioned that this is a greedy approach for merging cohorts).\n",
    "\n",
    "There are a few parameters used to control these validity checks:\n",
    "\n",
    "* ``min_cohort_size``: the minimum size a cohort is allowed to have to be considered valid\n",
    "* ``min_cohort_pct``: a value between [0, 1] that determines the minimum size allowed for a cohort. The minimum size is given by the size of the full dataset (``df.shape[0]``) multiplied by ``min_cohort_pct``. The maximum value between ``min_cohort_size`` and (``df.shape[0]`` * ``min_cohort_pct``) is  used to determine the minimum size allowed for a cohort\n",
    "* ``minority_min_rate``: the minimum occurrence rate for the minority class (from the label column) that a cohort is allowed to have. If the minority class of the cohort has an occurrence rate lower than min_rate, the cohort is considered invalid.\n",
    "\n",
    "In the next cell, we'll create a set of cohorts based on the joint values of the columns [\"age\", \"menopause\"]. We'll also specify a different value for the parameters ``min_cohort_pct`` and ``minority_min_rate``. Note that the resulting cohorts are not what we expected initially, that is, one cohort for each combination of unique values found between the columns [\"age\", \"menopause\"]. Instead, we end up with only a few cohorts. But note that while cohort ``cohort_4`` is a combination of simple filters based on these two columns, the other cohorts use a complex combination of filters based on these two columns. This means that the other two cohorts are a result of merged cohorts, and when two cohorts are merged, so are their filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COHORTS\n",
      "cohort_0:\n",
      "\tSize: 91\n",
      "\tQuery:\n",
      "\t\t((((((((`age` == \"20-29\") and (`menopause` == \"premeno\")) or ((`age` == \"30-39\") and (`menopause` == \"lt40\"))) or ((`age` == \"60-69\") and (`menopause` == \"lt40\"))) or ((`age` == \"50-59\") and (`menopause` == \"lt40\"))) or ((`age` == \"70-79\") and (`menopause` == \"ge40\"))) or ((`age` == \"40-49\") and (`menopause` == \"ge40\"))) or ((`age` == \"50-59\") and (`menopause` == \"premeno\"))) or ((`age` == \"30-39\") and (`menopause` == \"premeno\"))\n",
      "\tValue Counts:\n",
      "\t\t0: 59 (64.84%)\n",
      "\t\t1: 32 (35.16%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_4:\n",
      "\tSize: 81\n",
      "\tQuery:\n",
      "\t\t(`age` == \"40-49\") and (`menopause` == \"premeno\")\n",
      "\tValue Counts:\n",
      "\t\t0: 58 (71.60%)\n",
      "\t\t1: 23 (28.40%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_8:\n",
      "\tSize: 114\n",
      "\tQuery:\n",
      "\t\t((`age` == \"60-69\") and (`menopause` == \"ge40\")) or ((`age` == \"50-59\") and (`menopause` == \"ge40\"))\n",
      "\tValue Counts:\n",
      "\t\t0: 84 (73.68%)\n",
      "\t\t1: 30 (26.32%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing = [dp.EncoderOrdinal(verbose=False), dp.BasicImputer(verbose=False)]\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "\t\t\t\t\tcohort_col=[\"age\", \"menopause\"], \n",
    "\t\t\t\t\tmin_cohort_pct=0.2,\n",
    "\t\t\t\t\tminority_min_rate=0.15,\n",
    "\t\t\t\t\ttransform_pipe=preprocessing\n",
    "\t\t\t\t)\n",
    "dec_class.fit(df=df, label_col=\"Class\")\n",
    "\n",
    "dec_class.print_cohorts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Cohorts\n",
    "\n",
    "Just like the ``CohortManager`` class, the ``DecoupledClass`` also allows users to specify the exact filters they want when creating the cohorts. In the following example, we'll create three cohorts: 2 of them with specific filters, and the last one will be created to include all instances that don't belong to any other cohort. **NOTE:** when specifying the exact cohorts using the ``cohort_def`` parameter, invalid cohorts won't be merged. Instead, an error will be raised indicating that one of the cohorts is invalid. However, invalid cohorts can still be used if Transfer Learning is used. More details on that in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<raimitigations.cohort.decoupled_class.decoupled_classifier.DecoupledClass at 0x7f4c286b7160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COHORTS\n",
      "cohort_1:\n",
      "\tSize: 81\n",
      "\tQuery:\n",
      "\t\t(`age` == \"40-49\") and (`menopause` == \"premeno\")\n",
      "\tValue Counts:\n",
      "\t\t0: 58 (71.60%)\n",
      "\t\t1: 23 (28.40%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_2:\n",
      "\tSize: 90\n",
      "\tQuery:\n",
      "\t\t((`age` == \"60-69\") and (`menopause` == \"ge40\")) or ((`age` == \"30-39\") and (`menopause` == \"premeno\"))\n",
      "\tValue Counts:\n",
      "\t\t0: 58 (64.44%)\n",
      "\t\t1: 32 (35.56%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_3:\n",
      "\tSize: 115\n",
      "\tQuery:\n",
      "\t\t((`age` != \"40-49\") or (`menopause` != \"premeno\")) and (((`age` != \"60-69\") or (`menopause` != \"ge40\")) and ((`age` != \"30-39\") or (`menopause` != \"premeno\")))\n",
      "\tValue Counts:\n",
      "\t\t0: 85 (73.91%)\n",
      "\t\t1: 30 (26.09%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cohorts = {\n",
    "    \"cohort_1\": [['age', '==', '40-49'], 'and', ['menopause', '==', 'premeno']],\n",
    "\t\"cohort_2\": [\n",
    "            [['age', '==', '60-69'], 'and', ['menopause', '==', 'ge40']], 'or',\n",
    "            [['age', '==', '30-39'], 'and', ['menopause', '==', 'premeno']],\n",
    "        ],\n",
    "\t\"cohort_3\": None\n",
    "}\n",
    "\n",
    "preprocessing = [dp.EncoderOrdinal(verbose=False), dp.BasicImputer(verbose=False)]\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "\t\t\t\t\tcohort_def=cohorts, \n",
    "\t\t\t\t\tmin_cohort_pct=0.2,\n",
    "\t\t\t\t\tminority_min_rate=0.15,\n",
    "\t\t\t\t\ttransform_pipe=preprocessing\n",
    "\t\t\t\t)\n",
    "dec_class.fit(df=df, label_col=\"Class\")\n",
    "\n",
    "dec_class.print_cohorts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the estimator\n",
    "\n",
    "The ``DecoupledClass`` class has multiple parameters that allow for a wide range of customizations. For example, we can also choose the estimator used by the decoupled classifier. By default, we'll use a simple ``DecisionTreeClassifier`` (for classification problems) or ``DecisionTreeRegressor`` (for regression problems), both from `sklearn`. However, if the user wants to use a more powerful estimator or the same estimator, but tweak certain parameters of it, they can specify the estimator when creating the ``DecoupledClass`` object. To do this, they just need to instantiate the estimator (don't call their `fit()` method yet), and pass it through the ``estimator`` parameter. When doing this, the Decoupled Classifier will create a copy of this estimator for each cohort. This way, each estimator will be fitted using a different dataset (the cohort's subset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmendonca/anaconda3/envs/raipub/lib/python3.9/site-packages/xgboost/sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<raimitigations.cohort.decoupled_class.decoupled_classifier.DecoupledClass at 0x7f307a540a00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=30,\n",
    "            max_depth=10,\n",
    "            colsample_bytree=0.7,\n",
    "            alpha=0.0,\n",
    "            reg_lambda=10.0,\n",
    "            nthreads=4,\n",
    "            verbosity=0,\n",
    "            use_label_encoder=False,\n",
    "        )\n",
    "\n",
    "preprocessing = [dp.EncoderOrdinal(verbose=False), dp.BasicImputer(verbose=False)]\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "                    cohort_col=[\"age\", \"menopause\"], \n",
    "                    min_cohort_pct=0.2,\n",
    "                    minority_min_rate=0.15,\n",
    "                    estimator=model,\n",
    "                    transform_pipe=preprocessing\n",
    "                )\n",
    "dec_class.fit(df=df, label_col=\"Class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the predict() and predict_proba() methods\n",
    "\n",
    "The Decoupled Classifier also implements the same interface from other `sklearn`'s estimators: the ``predict()`` and ``predict_proba()`` methods. It also follows the same standards: the ``predict()`` method will return the exact classes, while the ``predict_proba()`` returns the probabilities of each instance belonging to each class. Note that the ``predict_proba()`` will only work if the estimator being used has the ``predict_proba()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred size = (286,)\n",
      "[0 0 0 0 0 0] ... [1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[label_col])\n",
    "\n",
    "y_pred = dec_class.predict(X)\n",
    "\n",
    "print(f\"y_pred size = {y_pred.shape}\")\n",
    "print(f\"{y_pred[:6]} ... {y_pred[-6:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred size = (286, 2)\n",
      "[[0.53159475 0.46840525]\n",
      " [0.78541195 0.21458806]\n",
      " [0.7623236  0.23767635]\n",
      " [0.8384228  0.1615772 ]\n",
      " [0.78541195 0.21458806]\n",
      " [0.8384228  0.1615772 ]] ... [[0.3676687  0.6323313 ]\n",
      " [0.72908753 0.27091247]\n",
      " [0.44569016 0.55430984]\n",
      " [0.81318545 0.18681458]\n",
      " [0.42785716 0.57214284]\n",
      " [0.45488244 0.54511756]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec_class.predict_proba(X)\n",
    "\n",
    "print(f\"y_pred size = {y_pred.shape}\")\n",
    "print(f\"{y_pred[:6]} ... {y_pred[-6:]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transfer Learning for invalid cohorts\n",
    "\n",
    "What sets the ``DecoupledClass`` apart from the ``CohortManager`` is its capability to deal with invalid cohorts. We already showed how to use the Decoupled Classifier to merge invalid cohorts using a greedy approach. In this section, we'll explore a second approach for dealing with invalid cohorts, called here the transfer learning approach. In this approach, instead of merging an invalid cohort into another cohort, we'll keep all the cohorts, but when calling the ``fit()`` method of an invalid cohort, we'll also use data from other cohorts (called out-data), but the instances of the outer-data will be weighed down compared to the instances belonging to the cohort (called here the in-data). Note that when an invalid cohort uses the data from other cohorts when calling its ``fit()`` method, the cohort that lent the data to the invalid cohort (the one from which the out-data was fetched) will still only use its own data when fitting its model (unless it is also an invalid cohort).\n",
    "\n",
    "We are now left with questions: (i) which cohorts should be used as the out-data for an invalid cohort, and (ii) how to define the value of $\\theta$.\n",
    "\n",
    "### Selecting the out-data for an invalid cohort\n",
    "\n",
    "When selecting the out-data for an invalid cohort, that is, when selecting which cohorts will be used to lend their data to the invalid cohort, we must be careful not to use cohorts with a very different label distribution (compared to the invalid cohort that needs extra data), otherwise, the use of external data can be more harmful than useful. To check if two cohorts have a similar label distribution (be it for a classification problem where the label column is a set of encoded classes, or for regression problems, where the label is an array of real values), we compute the Jensen-Shanon distance between the label distribution of these two cohorts. If the distance is below a predefined threshold (controlled by the ``cohort_dist_th`` parameter), then the distributions are considered similar.\n",
    "\n",
    "Here is a summary of the transfer learning process covered so far:\n",
    "\n",
    "* When using Transfer Learning, first check if there are any invalid cohorts. Differently from the greedy approach of merging cohorts, when using transfer learning, cohorts deemed invalid due to skewed distributions are not allowed. If a cohort is deemed invalid due to a skewed distribution, an error will be raised. For each invalid cohort `i`, we'll do the following steps:\n",
    "\n",
    "    1. Search all other cohorts `j` $\\neq$ `i` (including other invalid cohorts) and find those that have a similar label distribution\n",
    "    2. Create a new dataset (visible only to cohort `i`) called *out-data* that will include the subset of all other cohorts with a similar label distribution\n",
    "    3. Train the estimator of cohort `i` using its own subset (*in-data*) + *out-data*, where the instances from the out-data have a smaller weight $\\theta$ (we'll discuss how to set this value in the remainder of this section). Note that the estimator used when using transfer learning must allow for setting a different weight for each instance.\n",
    "\n",
    "* All valid cohorts will be trained using only their *in-data*, even if the subset of these cohorts is used as *out-data* for invalid cohorts.\n",
    "\n",
    "### Setting the value of $\\theta$\n",
    "\n",
    "We are now going to focus on how to set the value of the $\\theta$ parameter. We'll cover the different approaches for setting this value.\n",
    "\n",
    "#### Using a fixed $\\theta$ value\n",
    "\n",
    "The most straightforward approach for setting the value of $\\theta$ is to provide a specific value for it directly. This can be done using the ``theta`` parameter when creating the ``DecoupledClass`` object. When passing a float value between [0, 1] to this parameter, this will be the value used for $\\theta$ for all transfer learning operations.\n",
    "\n",
    "In the following cell, we'll set $\\theta$ = 0.3. Note that this time we first remove any missing values from the dataset. We do this because when using the ``cohort_col`` parameter to define the cohorts, if the columns used in this list have missing values, these values will be used for creating the cohorts. In this specific case, the ``breast-quad`` column has very few missing values, so if we try to create a set of cohorts with those missing values, this will result in a cohort (the one that holds all instances where this column is NaN) with a skewed label distribution, which, as previously mentioned, is not allowed when using transfer learning. Therefore, we simply remove the missing values prior to creating the cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COHORTS\n",
      "cohort_0:\n",
      "\tSize: 21\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"central\")\n",
      "\tValue Counts:\n",
      "\t\t0: 17 (80.95%)\n",
      "\t\t1: 4 (19.05%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_1', 'cohort_2', 'cohort_3', 'cohort_4']\n",
      "\t\tTheta = 0.3\n",
      "\n",
      "\n",
      "cohort_1:\n",
      "\tSize: 111\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"left_low\")\n",
      "\tValue Counts:\n",
      "\t\t0: 75 (67.57%)\n",
      "\t\t1: 36 (32.43%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_2:\n",
      "\tSize: 97\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"left_up\")\n",
      "\tValue Counts:\n",
      "\t\t0: 71 (73.20%)\n",
      "\t\t1: 26 (26.80%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_3:\n",
      "\tSize: 24\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"right_low\")\n",
      "\tValue Counts:\n",
      "\t\t0: 18 (75.00%)\n",
      "\t\t1: 6 (25.00%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_0', 'cohort_1', 'cohort_2', 'cohort_4']\n",
      "\t\tTheta = 0.3\n",
      "\n",
      "\n",
      "cohort_4:\n",
      "\tSize: 33\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"right_up\")\n",
      "\tValue Counts:\n",
      "\t\t0: 20 (60.61%)\n",
      "\t\t1: 13 (39.39%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_0', 'cohort_1', 'cohort_2', 'cohort_3']\n",
      "\t\tTheta = 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing = [dp.EncoderOrdinal(verbose=False)]\n",
    "\n",
    "imputer = dp.BasicImputer(categorical={'missing_values':np.nan, \n",
    "                                        'strategy':'most_frequent', \n",
    "                                        'fill_value':None },\n",
    "                            verbose=False)\n",
    "imputer.fit(df)\n",
    "df_nomiss = imputer.transform(df)\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "                    cohort_col=[\"breast-quad\"], \n",
    "                    theta=0.3,\n",
    "                    min_cohort_pct=0.2,\n",
    "                    minority_min_rate=0.15,\n",
    "                    transform_pipe=preprocessing\n",
    "                )\n",
    "dec_class.fit(df=df_nomiss, label_col=\"Class\")\n",
    "\n",
    "dec_class.print_cohorts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when using transfer learning and calling the ``print_cohorts()`` method, the \"Invalid\" key of the invalid cohorts will be set to True, and in that case, it will also inform which cohorts were used as out-data and the $\\theta$ value used.\n",
    "\n",
    "#### Finding the best $\\theta$ parameter using Cross-Validation\n",
    "\n",
    "Instead of using a fixed $\\theta$ value, we can also find the best value using Cross-Validation (CV). When a cohort uses transfer learning, CV is used with the cohort data (in-data) plus the out-data using different values of $\\theta$ (obtained from a list of $\\theta$ values, called here *$\\theta$ list*), and the final $\\theta$ is selected as being the one associated with the highest performance in the CV process. The CV here splits the in-data into K folds (the best K value is identified according to the possible values specified in the ``valid_k_folds_theta`` parameter), and then proceeds to use one of the folds as the test set, and the remaining folds plus the out-data as the train set. A model is fitted for the train set and then evaluated in the test set. The ROC AUC metric is obtained for each CV run until all folds have been used as a test set. We then compute the average ROC AUC score for the K runs and that gives the CV score for a given $\\theta$ value. This is repeated for all possible $\\theta$ values (the $\\theta$ list), and the $\\theta$ with the best score is selected for that cohort. This process is repeated for each cohort that requires transfer learning, which means that some invalid cohorts might end up using different values of $\\theta$.\n",
    "\n",
    "There are a set of parameters used for controlling the CV process. These parameters are: ``default_theta``, ``min_fold_size_theta``, and ``valid_k_folds_theta``. We recommend looking through the API documentation of these parameters to better understand this process.\n",
    "\n",
    "In the following cells, we'll check two ways to specify the **$\\theta$ list**, that is, the list of possible $\\theta$ values to be tested during the CV phase.\n",
    "\n",
    "##### Using a specific list of possible $\\theta$ values\n",
    "\n",
    "We can specify a list of possible $\\theta$ values. This way, when running the CV process mentioned above, we'll do it for all the $\\theta$ values contained in the list passed as a parameter. This list is passed to the same ``theta`` parameter mentioned in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COHORTS\n",
      "cohort_0:\n",
      "\tSize: 21\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"central\")\n",
      "\tValue Counts:\n",
      "\t\t0: 17 (80.95%)\n",
      "\t\t1: 4 (19.05%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_1', 'cohort_2', 'cohort_3', 'cohort_4']\n",
      "\t\tTheta = 0.6\n",
      "\n",
      "\n",
      "cohort_1:\n",
      "\tSize: 111\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"left_low\")\n",
      "\tValue Counts:\n",
      "\t\t0: 75 (67.57%)\n",
      "\t\t1: 36 (32.43%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_2:\n",
      "\tSize: 97\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"left_up\")\n",
      "\tValue Counts:\n",
      "\t\t0: 71 (73.20%)\n",
      "\t\t1: 26 (26.80%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_3:\n",
      "\tSize: 24\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"right_low\")\n",
      "\tValue Counts:\n",
      "\t\t0: 18 (75.00%)\n",
      "\t\t1: 6 (25.00%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_0', 'cohort_1', 'cohort_2', 'cohort_4']\n",
      "\t\tTheta = 0.8\n",
      "\n",
      "\n",
      "cohort_4:\n",
      "\tSize: 33\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"right_up\")\n",
      "\tValue Counts:\n",
      "\t\t0: 20 (60.61%)\n",
      "\t\t1: 13 (39.39%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_0', 'cohort_1', 'cohort_2', 'cohort_3']\n",
      "\t\tTheta = 0.8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dec_class = DecoupledClass(\n",
    "\t\t\t\t\tcohort_col=[\"breast-quad\"], \n",
    "\t\t\t\t\ttheta=[0.2, 0.4, 0.6, 0.8],\n",
    "\t\t\t\t\tmin_fold_size_theta=5,\n",
    "\t\t\t\t\tmin_cohort_pct=0.2,\n",
    "\t\t\t\t\tminority_min_rate=0.15,\n",
    "\t\t\t\t\ttransform_pipe=preprocessing\n",
    "\t\t\t\t)\n",
    "dec_class.fit(df=df_nomiss, label_col=\"Class\")\n",
    "\n",
    "dec_class.print_cohorts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a default list of possible $\\theta$ values\n",
    "\n",
    "Instead of providing a list of $\\theta$ values, we could also use a default $\\theta$ list. To do this, we only need to set the ``theta`` parameter to ``True``. This way, the ``DecoupledClass`` understands that transfer learning must be used, and that the best $\\theta$ value must be identified using a default $\\theta$ list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COHORTS\n",
      "cohort_0:\n",
      "\tSize: 21\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"central\")\n",
      "\tValue Counts:\n",
      "\t\t0: 17 (80.95%)\n",
      "\t\t1: 4 (19.05%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_1', 'cohort_2', 'cohort_3', 'cohort_4']\n",
      "\t\tTheta = 0.2\n",
      "\n",
      "\n",
      "cohort_1:\n",
      "\tSize: 111\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"left_low\")\n",
      "\tValue Counts:\n",
      "\t\t0: 75 (67.57%)\n",
      "\t\t1: 36 (32.43%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_2:\n",
      "\tSize: 97\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"left_up\")\n",
      "\tValue Counts:\n",
      "\t\t0: 71 (73.20%)\n",
      "\t\t1: 26 (26.80%)\n",
      "\tInvalid: False\n",
      "\n",
      "\n",
      "cohort_3:\n",
      "\tSize: 24\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"right_low\")\n",
      "\tValue Counts:\n",
      "\t\t0: 18 (75.00%)\n",
      "\t\t1: 6 (25.00%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_0', 'cohort_1', 'cohort_2', 'cohort_4']\n",
      "\t\tTheta = 0.4\n",
      "\n",
      "\n",
      "cohort_4:\n",
      "\tSize: 33\n",
      "\tQuery:\n",
      "\t\t(`breast-quad` == \"right_up\")\n",
      "\tValue Counts:\n",
      "\t\t0: 20 (60.61%)\n",
      "\t\t1: 13 (39.39%)\n",
      "\tInvalid: True\n",
      "\t\tCohorts used as outside data: ['cohort_0', 'cohort_1', 'cohort_2', 'cohort_3']\n",
      "\t\tTheta = 0.2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dec_class = DecoupledClass(\n",
    "\t\t\t\t\tcohort_col=[\"breast-quad\"], \n",
    "\t\t\t\t\ttheta=True,\n",
    "\t\t\t\t\tmin_fold_size_theta=5,\n",
    "\t\t\t\t\tmin_cohort_pct=0.2,\n",
    "\t\t\t\t\tminority_min_rate=0.15,\n",
    "\t\t\t\t\ttransform_pipe=preprocessing\n",
    "\t\t\t\t)\n",
    "dec_class.fit(df=df_nomiss, label_col=\"Class\")\n",
    "\n",
    "dec_class.print_cohorts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Fairness Metrics\n",
    "\n",
    "Aside from training one estimator for each cohort and adding transfer learning for invalid cohorts, the Decoupled Classifier, as presented in its [original paper](https://www.microsoft.com/en-us/research/publication/decoupled-classifiers-for-group-fair-and-efficient-machine-learning/), also offers the option to optimize all models according to some fairness metric. In this section, we'll show how to optimize a set of models based on one of the fairness metrics available.\n",
    "\n",
    "First of all, let's start by reading a new dataset, and then splitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>region</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>recruitment_channel</th>\n",
       "      <th>no_of_trainings</th>\n",
       "      <th>age</th>\n",
       "      <th>previous_year_rating</th>\n",
       "      <th>length_of_service</th>\n",
       "      <th>KPIs_met &gt;80%</th>\n",
       "      <th>awards_won?</th>\n",
       "      <th>avg_training_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19004</th>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_13</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54186</th>\n",
       "      <td>Technology</td>\n",
       "      <td>region_2</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37539</th>\n",
       "      <td>Operations</td>\n",
       "      <td>region_2</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51713</th>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_22</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>f</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>Procurement</td>\n",
       "      <td>region_2</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>f</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51522</th>\n",
       "      <td>Operations</td>\n",
       "      <td>region_27</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>Operations</td>\n",
       "      <td>region_32</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24351</th>\n",
       "      <td>Operations</td>\n",
       "      <td>region_2</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9214</th>\n",
       "      <td>Procurement</td>\n",
       "      <td>region_17</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>f</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54230</th>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_11</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38365 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              department     region         education gender  \\\n",
       "19004  Sales & Marketing  region_13        Bachelor's      m   \n",
       "54186         Technology   region_2        Bachelor's      f   \n",
       "37539         Operations   region_2        Bachelor's      m   \n",
       "51713  Sales & Marketing  region_22        Bachelor's      f   \n",
       "2051         Procurement   region_2        Bachelor's      f   \n",
       "...                  ...        ...               ...    ...   \n",
       "51522         Operations  region_27        Bachelor's      f   \n",
       "4220          Operations  region_32        Bachelor's      m   \n",
       "24351         Operations   region_2  Master's & above      m   \n",
       "9214         Procurement  region_17        Bachelor's      f   \n",
       "54230  Sales & Marketing  region_11        Bachelor's      m   \n",
       "\n",
       "      recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
       "19004               other                1   27                   3.0   \n",
       "54186               other                2   38                   4.0   \n",
       "37539            sourcing                1   26                   1.0   \n",
       "51713            sourcing                1   27                   2.0   \n",
       "2051             sourcing                1   28                   3.0   \n",
       "...                   ...              ...  ...                   ...   \n",
       "51522               other                1   33                   3.0   \n",
       "4220                other                1   33                   2.0   \n",
       "24351            sourcing                1   31                   5.0   \n",
       "9214             sourcing                1   43                   3.0   \n",
       "54230            sourcing                1   34                   1.0   \n",
       "\n",
       "       length_of_service  KPIs_met >80%  awards_won?  avg_training_score  \n",
       "19004                  4              0            0                  52  \n",
       "54186                 10              1            0                  79  \n",
       "37539                  3              0            0                  57  \n",
       "51713                  4              0            0                  49  \n",
       "2051                   2              0            0                  66  \n",
       "...                  ...            ...          ...                 ...  \n",
       "51522                 10              1            0                  62  \n",
       "4220                   4              0            0                  62  \n",
       "24351                  3              1            0                  64  \n",
       "9214                   4              1            0                  66  \n",
       "54230                  6              0            0                  51  \n",
       "\n",
       "[38365 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "sys.path.append('../../notebooks')\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from download import download_datasets\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def get_model():\n",
    "    model = LGBMClassifier(random_state=SEED)\n",
    "    return model\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "data_dir = '../../../datasets/'\n",
    "download_datasets(data_dir)\n",
    "df =  pd.read_csv(data_dir + 'hr_promotion/train.csv')\n",
    "df.drop(columns=['employee_id'], inplace=True)\n",
    "label_col = 'is_promoted'\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(df, label_col, test_size=0.3)\n",
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoupled Classifier WITHOUT Fairness Optimization\n",
    "\n",
    "We'll look into the cohorts formed by the \"department\" column. We'll first train a decoupled classifier without optimizing any fairness metric, and then output the results. Note here that we're using the ``get_threasholds_dict()`` method from the **DecoupledClass**, which returns a dictionary with all the optimal thresholds found for the training set for each cohort. These thresholds are used only for binary classification problems, and they are used whenever the user uses the ``predict()`` method: in this case, we'll first compute the probabilities (``predict_proba()``), and then we binarize the results using this threshold as the cutoff point (values below the threshold are assigned the ``0`` class, and values above are assigned the ``1`` class). The dictionary returned by the ``get_threasholds_dict()`` method can be used when calling the ``raimitigations.cohort.fetch_cohort_results()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>cht_query</th>\n",
       "      <th>roc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_pos</th>\n",
       "      <th>%_pos</th>\n",
       "      <th>cht_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.876146</td>\n",
       "      <td>0.685779</td>\n",
       "      <td>0.742239</td>\n",
       "      <td>0.939488</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>659</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>16443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cohort_0</td>\n",
       "      <td>(`department` == \"Analytics\")</td>\n",
       "      <td>0.722190</td>\n",
       "      <td>0.619585</td>\n",
       "      <td>0.596869</td>\n",
       "      <td>0.606264</td>\n",
       "      <td>0.875456</td>\n",
       "      <td>0.291690</td>\n",
       "      <td>126</td>\n",
       "      <td>0.076549</td>\n",
       "      <td>1646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohort_1</td>\n",
       "      <td>(`department` == \"Finance\")</td>\n",
       "      <td>0.919360</td>\n",
       "      <td>0.838021</td>\n",
       "      <td>0.786997</td>\n",
       "      <td>0.809857</td>\n",
       "      <td>0.943570</td>\n",
       "      <td>0.399871</td>\n",
       "      <td>56</td>\n",
       "      <td>0.073491</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cohort_2</td>\n",
       "      <td>(`department` == \"HR\")</td>\n",
       "      <td>0.895265</td>\n",
       "      <td>0.836806</td>\n",
       "      <td>0.697395</td>\n",
       "      <td>0.746324</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.547408</td>\n",
       "      <td>24</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cohort_3</td>\n",
       "      <td>(`department` == \"Legal\")</td>\n",
       "      <td>0.915327</td>\n",
       "      <td>0.841469</td>\n",
       "      <td>0.663451</td>\n",
       "      <td>0.717749</td>\n",
       "      <td>0.963190</td>\n",
       "      <td>0.760291</td>\n",
       "      <td>7</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cohort_4</td>\n",
       "      <td>(`department` == \"Operations\")</td>\n",
       "      <td>0.896737</td>\n",
       "      <td>0.757878</td>\n",
       "      <td>0.729081</td>\n",
       "      <td>0.742328</td>\n",
       "      <td>0.918598</td>\n",
       "      <td>0.271413</td>\n",
       "      <td>272</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cohort_5</td>\n",
       "      <td>(`department` == \"Procurement\")</td>\n",
       "      <td>0.901670</td>\n",
       "      <td>0.758723</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>0.736035</td>\n",
       "      <td>0.915596</td>\n",
       "      <td>0.329608</td>\n",
       "      <td>173</td>\n",
       "      <td>0.079358</td>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cohort_6</td>\n",
       "      <td>(`department` == \"R&amp;D\")</td>\n",
       "      <td>0.718231</td>\n",
       "      <td>0.578056</td>\n",
       "      <td>0.526861</td>\n",
       "      <td>0.535669</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.664189</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cohort_7</td>\n",
       "      <td>(`department` == \"Sales &amp; Marketing\")</td>\n",
       "      <td>0.935592</td>\n",
       "      <td>0.765992</td>\n",
       "      <td>0.727787</td>\n",
       "      <td>0.745056</td>\n",
       "      <td>0.938333</td>\n",
       "      <td>0.272557</td>\n",
       "      <td>298</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cohort_8</td>\n",
       "      <td>(`department` == \"Technology\")</td>\n",
       "      <td>0.869781</td>\n",
       "      <td>0.710383</td>\n",
       "      <td>0.682738</td>\n",
       "      <td>0.695082</td>\n",
       "      <td>0.886847</td>\n",
       "      <td>0.329839</td>\n",
       "      <td>197</td>\n",
       "      <td>0.095261</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort                              cht_query       roc  precision  \\\n",
       "0       all                                    all  0.895163   0.876146   \n",
       "1  cohort_0          (`department` == \"Analytics\")  0.722190   0.619585   \n",
       "2  cohort_1            (`department` == \"Finance\")  0.919360   0.838021   \n",
       "3  cohort_2                 (`department` == \"HR\")  0.895265   0.836806   \n",
       "4  cohort_3              (`department` == \"Legal\")  0.915327   0.841469   \n",
       "5  cohort_4         (`department` == \"Operations\")  0.896737   0.757878   \n",
       "6  cohort_5        (`department` == \"Procurement\")  0.901670   0.758723   \n",
       "7  cohort_6                (`department` == \"R&D\")  0.718231   0.578056   \n",
       "8  cohort_7  (`department` == \"Sales & Marketing\")  0.935592   0.765992   \n",
       "9  cohort_8         (`department` == \"Technology\")  0.869781   0.710383   \n",
       "\n",
       "     recall        f1  accuracy  threshold  num_pos     %_pos  cht_size  \n",
       "0  0.685779  0.742239  0.939488   0.500000      659  0.040078     16443  \n",
       "1  0.596869  0.606264  0.875456   0.291690      126  0.076549      1646  \n",
       "2  0.786997  0.809857  0.943570   0.399871       56  0.073491       762  \n",
       "3  0.697395  0.746324  0.956989   0.547408       24  0.032258       744  \n",
       "4  0.663451  0.717749  0.963190   0.760291        7  0.021472       326  \n",
       "5  0.729081  0.742328  0.918598   0.271413      272  0.080808      3366  \n",
       "6  0.718070  0.736035  0.915596   0.329608      173  0.079358      2180  \n",
       "7  0.526861  0.535669  0.944444   0.664189        5  0.015432       324  \n",
       "8  0.727787  0.745056  0.938333   0.272557      298  0.059280      5027  \n",
       "9  0.682738  0.695082  0.886847   0.329839      197  0.095261      2068  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing = [dp.BasicImputer(verbose=False), dp.EncoderOrdinal(verbose=False)]\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "                    cohort_col=[\"department\"],\n",
    "                    transform_pipe=preprocessing,\n",
    "                    estimator=get_model(),\n",
    "                    minority_min_rate=0.01,\n",
    "                    min_cohort_pct=0.01,\n",
    "                    theta=False,\n",
    "                )\n",
    "dec_class.fit(X_train, y_train)\n",
    "    \n",
    "th_dict = dec_class.get_threasholds_dict()\n",
    "pred = dec_class.predict_proba(X_test)\n",
    "fetch_cohort_results(X_test, y_test, pred, cohort_def=dec_class, fixed_th=th_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoupled Classifier WITH Fairness Optimization\n",
    "\n",
    "Now, we'll add the **Demographic Parity** fairness function (refer to the paper for other loss functions). The demographic parity loss forces the decoupled classifier to output a similar rate of positive labels for all cohorts. The Demographic Parity is given by $\\hat{L}$, expressed in the following equations:\n",
    "\n",
    "$$ L_{1} = \\frac{1}{n} \\sum_{i = 0...n}{|y_i - z_i|} $$\n",
    "$$ p_k = \\frac{1}{n} \\sum_{i:g_i = k}{z_i} $$\n",
    "$$ \\hat{L} = \\lambda L_{1} + (1-\\lambda) \\sum_{k}{\\Bigg| p_k \\frac{n}{n_k} - \\frac{1}{K}\\sum_{k'}{p_{k'} \\frac{n}{n_{k'}}} \\Bigg|} $$\n",
    "\n",
    "where:\n",
    "\n",
    "* $L_1$ is the L1 loss function\n",
    "* $p_k$ is the rate of positive labels (in relation to the entire dataset) inside cohort $k$\n",
    "* $\\sum_{k'}$ represents the sum over all cohorts different from cohort $k$\n",
    "\n",
    "To use the **Demographic Parity** loss, we'll use three new parameters:\n",
    "\n",
    "* **fairness_loss**: the fairness loss that should be optimized alongside the L1 loss.\n",
    "  This is only possible for binary classification problems. For regression or multi-class\n",
    "  problems, this parameter should be set to None (default value), otherwise, an error will\n",
    "  be raised. The L1 and fairness losses are computed over the binarized predictions, not\n",
    "  over the probabilities. Therefore, the decoupled classifier tries to identify the best\n",
    "  set of thresholds (one for each estimator, where we have one estimator for each cohort)\n",
    "  that produces the lowest joint loss (L1 + fairness loss). There are 3 available fairness\n",
    "  losses:\n",
    "\n",
    "    * ``None``: don't use any fairness loss. The threshold used for each cohort is identified through the ROC curve, that is, doesn't consider any  fairness metric. This is the default behavior;\n",
    "    * \"balanced\": the Balanced loss is computed as the mean loss value over the L1 loss of each cohort. This loss is useful when we want that all cohorts to have a similar L1 loss, where all cohorts are considered with an equal weight, which makes it ideal for unbalanced datasets;\n",
    "    * \"num_parity\": the Numerical Parity loss forces all cohorts to have a similar number of positive labels. This loss is useful in a situation where a model should output an equal number of positive labels for each cohort;\n",
    "    * \"dem_parity\": the Demographic Parity loss forces all cohorts to have a similar rate of positive labels. This is somehow similar to the Numerical Parity loss, but this loss accounts for the difference in size of each cohort, that is, the number of positive labels should be different for cohorts with different sizes, but the ratio of positive labels over the size of the cohort should be consistent across cohorts. This is useful when we want an unbiased model, that is, a model that outputs an equal proportion of positive labels without considering the cohort to which an instance belongs to;\n",
    "\n",
    "* **lambda_coef**: the $\\lambda$ variable presented in the equations above, which represents the weight assigned to the L1 loss when computing the joint loss (L1 + fairness loss). This parameter is ignored when ``fairness_loss = None``;\n",
    "* **max_joint_loss_time**: the maximum time (in seconds) allowed for the decoupled classifier to run its fairness optimization step. This parameter is ignored when ``fairness_loss = None``. When ``fairness != None``, the decoupled classifier will try to find the best set of thresholds to be used for each cohort such that the final predictions result in a minimum joint loss. However, this optimization step is computationally expensive, and can take some time to be finalized depending on the number of cohorts and the size of the dataset. To avoid long execution times, we can specify the maximum time allowed for the decoupled classifier to run this step. If the optimization step reaches the maximum time, then the best set of thresholds found so far is returned;\n",
    "\n",
    "Finally, another new trick we'll use is to call the ``fetch_cohort_results()`` function using ``fixed_th = True``. What this will do is that it will use the thresholds from the DecoupledClass object passed through the ``cohort_def`` parameter (using the ``get_threasholds_dict()`` method used in the previous cell). Using ``fixed_th = True`` only works if we provide a DecoupledClass object through the ``cohort_def`` parameter. Otherwise, an error will be raised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>cht_query</th>\n",
       "      <th>roc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_pos</th>\n",
       "      <th>%_pos</th>\n",
       "      <th>cht_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.876146</td>\n",
       "      <td>0.685779</td>\n",
       "      <td>0.742239</td>\n",
       "      <td>0.939488</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>659</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>16443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cohort_0</td>\n",
       "      <td>(`department` == \"Analytics\")</td>\n",
       "      <td>0.722190</td>\n",
       "      <td>0.636853</td>\n",
       "      <td>0.583599</td>\n",
       "      <td>0.600360</td>\n",
       "      <td>0.886999</td>\n",
       "      <td>0.365011</td>\n",
       "      <td>93</td>\n",
       "      <td>0.056501</td>\n",
       "      <td>1646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohort_1</td>\n",
       "      <td>(`department` == \"Finance\")</td>\n",
       "      <td>0.919360</td>\n",
       "      <td>0.878382</td>\n",
       "      <td>0.762182</td>\n",
       "      <td>0.807264</td>\n",
       "      <td>0.947507</td>\n",
       "      <td>0.579517</td>\n",
       "      <td>45</td>\n",
       "      <td>0.059055</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cohort_2</td>\n",
       "      <td>(`department` == \"HR\")</td>\n",
       "      <td>0.895265</td>\n",
       "      <td>0.655064</td>\n",
       "      <td>0.741758</td>\n",
       "      <td>0.685869</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>0.051923</td>\n",
       "      <td>68</td>\n",
       "      <td>0.091398</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cohort_3</td>\n",
       "      <td>(`department` == \"Legal\")</td>\n",
       "      <td>0.915327</td>\n",
       "      <td>0.630507</td>\n",
       "      <td>0.820043</td>\n",
       "      <td>0.672362</td>\n",
       "      <td>0.898773</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>40</td>\n",
       "      <td>0.122699</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cohort_4</td>\n",
       "      <td>(`department` == \"Operations\")</td>\n",
       "      <td>0.896737</td>\n",
       "      <td>0.759269</td>\n",
       "      <td>0.724897</td>\n",
       "      <td>0.740439</td>\n",
       "      <td>0.918895</td>\n",
       "      <td>0.278185</td>\n",
       "      <td>265</td>\n",
       "      <td>0.078728</td>\n",
       "      <td>3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cohort_5</td>\n",
       "      <td>(`department` == \"Procurement\")</td>\n",
       "      <td>0.901670</td>\n",
       "      <td>0.812134</td>\n",
       "      <td>0.709188</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.418526</td>\n",
       "      <td>135</td>\n",
       "      <td>0.061927</td>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cohort_6</td>\n",
       "      <td>(`department` == \"R&amp;D\")</td>\n",
       "      <td>0.718231</td>\n",
       "      <td>0.578056</td>\n",
       "      <td>0.526861</td>\n",
       "      <td>0.535669</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.664189</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cohort_7</td>\n",
       "      <td>(`department` == \"Sales &amp; Marketing\")</td>\n",
       "      <td>0.935592</td>\n",
       "      <td>0.731392</td>\n",
       "      <td>0.742891</td>\n",
       "      <td>0.736963</td>\n",
       "      <td>0.929779</td>\n",
       "      <td>0.231286</td>\n",
       "      <td>371</td>\n",
       "      <td>0.073801</td>\n",
       "      <td>5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cohort_8</td>\n",
       "      <td>(`department` == \"Technology\")</td>\n",
       "      <td>0.869781</td>\n",
       "      <td>0.775429</td>\n",
       "      <td>0.668209</td>\n",
       "      <td>0.704017</td>\n",
       "      <td>0.904739</td>\n",
       "      <td>0.418906</td>\n",
       "      <td>134</td>\n",
       "      <td>0.064797</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort                              cht_query       roc  precision  \\\n",
       "0       all                                    all  0.895163   0.876146   \n",
       "1  cohort_0          (`department` == \"Analytics\")  0.722190   0.636853   \n",
       "2  cohort_1            (`department` == \"Finance\")  0.919360   0.878382   \n",
       "3  cohort_2                 (`department` == \"HR\")  0.895265   0.655064   \n",
       "4  cohort_3              (`department` == \"Legal\")  0.915327   0.630507   \n",
       "5  cohort_4         (`department` == \"Operations\")  0.896737   0.759269   \n",
       "6  cohort_5        (`department` == \"Procurement\")  0.901670   0.812134   \n",
       "7  cohort_6                (`department` == \"R&D\")  0.718231   0.578056   \n",
       "8  cohort_7  (`department` == \"Sales & Marketing\")  0.935592   0.731392   \n",
       "9  cohort_8         (`department` == \"Technology\")  0.869781   0.775429   \n",
       "\n",
       "     recall        f1  accuracy  threshold  num_pos     %_pos  cht_size  \n",
       "0  0.685779  0.742239  0.939488   0.500000      659  0.040078     16443  \n",
       "1  0.583599  0.600360  0.886999   0.365011       93  0.056501      1646  \n",
       "2  0.762182  0.807264  0.947507   0.579517       45  0.059055       762  \n",
       "3  0.741758  0.685869  0.913978   0.051923       68  0.091398       744  \n",
       "4  0.820043  0.672362  0.898773   0.033636       40  0.122699       326  \n",
       "5  0.724897  0.740439  0.918895   0.278185      265  0.078728      3366  \n",
       "6  0.709188  0.747522  0.926606   0.418526      135  0.061927      2180  \n",
       "7  0.526861  0.535669  0.944444   0.664189        5  0.015432       324  \n",
       "8  0.742891  0.736963  0.929779   0.231286      371  0.073801      5027  \n",
       "9  0.668209  0.704017  0.904739   0.418906      134  0.064797      2068  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing = [dp.BasicImputer(verbose=False), dp.EncoderOrdinal(verbose=False)]\n",
    "\n",
    "dec_class = DecoupledClass(\n",
    "                    cohort_col=[\"department\"],\n",
    "                    transform_pipe=preprocessing,\n",
    "                    estimator=get_model(),\n",
    "                    minority_min_rate=0.01,\n",
    "                    min_cohort_pct=0.01,\n",
    "                    theta=False,\n",
    "                    fairness_loss=\"dem_parity\",\n",
    "                    lambda_coef=0.5,\n",
    "                    max_joint_loss_time=200\n",
    "                )\n",
    "dec_class.fit(X_train, y_train)\n",
    "    \n",
    "pred = dec_class.predict_proba(X_test)\n",
    "fetch_cohort_results(X_test, y_test, pred, cohort_def=dec_class, fixed_th=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, when using Demographic Parity with $\\lambda = 0.5$, the percentage of positive labels inside each cohort is more equal across different cohorts when compared to the previous result. This is important if the generated model must not be biased for a given group, that is, provide equal rates of positive labels for each cohort, even if this hurts the overall performance of the model. The $\\lambda$ parameter controls how much we should care about the fairness metric compared to the L1 loss: low $\\lambda$ values focuses more on the fairness metric, while high $\\lambda$ values gives more attention to the L1 loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "239972f3e2cce5deeefd34fe5d6dd927057b08952d864776048f8db51d1a831a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
